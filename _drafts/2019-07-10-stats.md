---
title:  "Interview Study Guide Part I: Statistics"
category: posts
date: 2019-07-10
---

As I have gone through a couple rounds of interviews for data scientist
positions, I've been compiling notes on what I consider to be the essential
areas of knowledge. I want to make these notes available to the general public;
although there are many blog posts out there that are supposed to help one
prepare for data science interviews, I haven't found any of them to be very
high-quality. 

From my perspective, there are four key subject areas that a data scientist
should feel comfortable with when going into an interview:

1. Statistics (including experimental design)
2. Machine Learning
3. Software Engineering (including SQL)
4. "Soft" Questions

I'm going to go through each of these individually. This first post will focus
on statistics. We will go over a number of topics in statistics in no particular
order. Note that **this post will not teach you statistics; it will remind you
of what you should already know.**

If you're utterly unfamiliar with the concepts I'm mentioning, I'd recommend
[this excellent MIT course on probability & statistics][1] as a good starting
point. When I began interviewing, I had never taken a statistics class before; I
worked through the notes, homeworks, and exams for this course, and at the end
had a solid foundation to learn the specific things that you need to know for
these interviews.

Of course, this should all be taken with a grain of salt; I've been in the
industry a relatively short time, and haven't done very many interviews. I hope
to improve this guide over time; please let me know in the comments if there's
something you think should be added, removed, or changed!

# The Central Limit Theorem

The Central Limit Theorem is a fundamental tool in statistical analysis. It
state (roughly) that when you add up a bunch of random variables with finite
variance, then their sum will converge to a Gaussian distribution.[^fnote1]

What does this mean to a data scientist? Well, one place where we see a sum of
random variables is in a _sample mean_. One consequence of the central limit
theorem is that the sample mean of a variable with mean $$\mu$$ and variance
$$\sigma^2$$ will itself have mean $$\mu$$ and variance $$\sigma^2/n$$, where
$$n$$ is the number of saples.

This is cool! We don't need to know much of anything about the distribution of
we're sampling from, besides its mean and variance. This simplification will
allow us to do hypothesis testing that compares two means with relative ease.

## An Example

Suppose that we are sampling a Bernoulli random variable. This is a 0/1 random
variable that is 1 with probability $$p$$ and 0 with probability $$1-p$$. If we
get the sequence of ten draws $$[0,1,1,0,0,0,1,0,1,0]$$, then our sample mean is 

$$\hat \mu = \frac{1}{10}\sum_{i=1}^10 x_i = 0.4$$

Of course, this sample mean is itself a random variable - when we report it, we
would like to report an estimate on its variance as well. The central limit
theorem tells us that this will, as $$n$$ increases, converge to a Gaussian
distribution. Since the mean of the Bernoulli random variable is $$p$$ and its
variance is $$p(1-p)$$, we know that the distribution of the sample mean will
converge to a Gaussian with mean $$p$$ and variance $$p(1-p)/n$$. So we could
say that our estimate of the parameter $$p$$ is 0.4 $$\pm$$ 0.155. Of course,
we're playing a bit loose here, since we're using the estimate $$\hat p$$ from
the data, as we don't actually know the _true_ parameter $$p$$.

Now, a sample size of $$n=10$$ is a bit small to be relying on a "large-$$n$$"
result like the CLT. Actually, in this case, we know the exact distribution of
the sample mean, since $$\sum_i x_i$$ is binomially distributed with parameters
$$p$$ and $$n$$. 

## Other Questions on the CLT

I find that the CLT more comes up as a piece of context in other questions
rather than as something that gets asked about directly, but you should be
prepared to answer the following questions.

- **What is the central limit theorem?** We've addressed this above - I doubt
  they'll be expecting a mathematically-correct statement of the theorem, but
  you should know the gist of it, along with significant limitations (finite
  variance being the major one).

- **When can you _not_ use the CLT?** I think the key thing here is that you
  have to be normalizing the data in an appropriate way (dividing by the sample
  size), and that the underlying variance must be finite. The answer here can
  get very subtle and mathematical, involving modes of convergence for random
  variables and all that, but I doubt they will push you to go there, unless
  you're applying for a job specifically as a statistician.
  
- **Give me an example of the CLT in use.** The classic example here is the
  distribution of the sample mean converging to a normal distribution as the
  number of samples grows large.

# Hypothesis Testing

Hypothesis testing (also known by the more verbose "null hypothesis
significance testing") is a huge subject, both in scope and importance. We use
statistics to quantitatively answer questions based on data, and hypothesis
testing is one of the primary methods by which we construct these answers. Here
we'll just lay out one possible case of a hypothesis test. It's simple, but it
comes up all the time in practice, so it's essential to know.

## An Example

Suppose we have two buttons, one green and one blue. We put them in front of
two different samples of users. For simplicity, let's say that each sample has
size $$n=100$$. We observe that $$k_\text{green}$$ 57 users click the green
button, and only $$k_\text{blue} = 48$$ click the blue button.

Seems like the green button is better, right? Well, we want to be able to say
how _confident_ we are of this fact. We'll do this in the language of null
hypothesis significance testing. I'm going to lay out a table of all the
important factors here, and then discuss

| Description | Value |
|------------|-------|
| Null Hypothesis| $$p_{blue} - p_{green} = 0$$ |
| Test Statistic | $$ \frac{k_\text{blue}}{n} - \frac{k_\text{green}}{n} $$ |
| Test Statistic's Distribution | $$N(0, (p_b(1-p_b) + p_g(1-p_g)) / n)$$ |
| Test Statistic's Observed Value | -0.09 | 
| $$p$$-value | 0.1003 |

There are a few noteworthy things here. First, we really want to know whether
$$p_g > p_b$$, but that's equivalent to $$p_b-p_g < 0$$. Second, we assume that
$$n$$ is large enough so that $$k/n$$ is approximately normally distributed,
with mean $$\mu = p$$ and variance $$\sigma^2 = p(1-p)/n$$. Third, since the
differences of two normals is itself a normal, the test statistics distribution
is (under the null hypothesis) a normal with mean zero and the variance given
(which is the sum of the two variances of $$k_b/n$$ and $$k_g/n$$). 

Finally, we don't actually know $$p_b$$ or $$p_g$$, so we can't really compute
the $$p$$-value; what we do is we say that $$k_b/n$$ is "close enough"" to
$$p_b$$ and use it as an approximation. That gives us our final $$p$$-value.

The $$p$$-value was calculated in Python, as follows:

{% highlight python %}
from scipy.stats import norm
pb = 0.48
pg = 0.57
n = 100
sigma = np.sqrt((pb*(1-pb) + pg*(1-pg))/n)
norm.cdf(-0.09, loc = 0, scale = sigma) # 0.10034431272089045
{% endhighlight %}

Phew! I went through that pretty quick, but if you can't follow the gist of what
I was doing there, I'd recommend you think through it until it is clear to
you. You will be faced with more complicated situations in practice; it's
important that you begin by understanding the most simple situation inside out.

## Other Topics in Hypothesis Testing

Some important follow-up questions you should be able to answer:

- **What are Type I & II error? What is a situation where you would be more
  concerned with Type I error? Vice versa?** These are discussed [on
  Wikipedia][15]. Type I error is false-positive error. You might be very
  concerned with Type I error if you are interviewing job candidates, and there
  are many possible options; it is very costly to hire the wrong person for the
  job, so you really want to avoid false positives. Type II error is
  false-negative error. If you are testing for a disease that is deadly but has
  a simple cure, then you would certainly NOT want to have a false negative
  result of the test, since that would result in an easily-avoidable negative outcome.

- **What is the _power_ of a test? How do you calculate it?** To calculate the
  power, you need an alternative hypothesis; in the example above, this would
  look like $$p_b-p_g = -0.1$$. Although these alternative hypothesis are often
  somewhat ad-hoc, the power analysis depends critically upon them. Google will
  turn up plenty of videos and tutorials on calculating the power of a test

- **What is the significance of a test?** This is the same as the
  $$p$$-value threshold below which we reject the null
  hypothesis. (In)famously, 0.05 has become the de-facto standard throughout
  many sciences for significance levels worthy of publication.

- **Gow would you explain a p-value to a lay person**? Of course, you should
  have a solid understanding of the statistical definition of the
  $$p$$-value. A generally accepted answer is "a $$p$$-value quantifies the
  evidence for a hypothesis - closer to zero means more evidence." Of course,
  this is wrong on a lot of levels - it's actually quantifying evidence
  _against_ the null hypothesis, not _for_ the alternative. For what it's
  worth, I'm not convinced there's a great answer to that one; it's an
  inherently technical quantity that is frequently misrepresented and abused by
  people trying to (falsely) simplify its meaning.

- **If you measure 14 different test statistics, and get a $$p$$-value for each
  (all based on the same null hypothesis), how do you combine them to get an
  aggregate $$p$$-value?** This one is more of a bonus question, but it's worth
  knowing. It's actually not obvious how do to this, and the true $$p$$-value
  depends on how the tests depend on each other. However, you can get an
  upper-bound (worst-case estimate) on the aggregate $$p$$-value by adding
  together the 14 different $$p$$-values. The validity of this bound results
  from the inclusion-exclusion principle.
  
# Confidence Intervals

Confidence intervals allow us to say how certain we are of a result. If we
count that 150 out of 400 people sample randomly from a city identify
themselves as male, then our best estimate of the fraction of women in the city
is 250/400, or 5/8. How certain are we of this fact?[^fnotea]

Suppose that we want to find a 95% confidence inverval on the female fraction
in the city discussed above. This corresponds to a significance level of
$$\alpha/2$$. There are a few different ways to generate confidence intervals.

## The Exact Method

To get the **exact confidence inverval**, we'd need to invert the CDF to find
where it hits $$\alpha/2$$ and $$1-\alpha/2$$. That is, we need to find the
value $$p_l$$ that solves the equation

$$CDF\left(n, p_l\right) = \alpha/2$$

and the value $$p_u$$ that solves the equation

$$CDF\left(n, p_u\right) = 1 - \alpha/2.$$

In these, $$CDF(n,p)$$ is the cumulative distribution function of a binomial
random variable with parameters $$n$$ and $$p.$$ Solving the two equations
above would give us our confidence inverval $$[p_l, p_u]$$.

Although it is useful for theoretical analysis, I rarely use this method in
practice, because I often do not actually know the true CDF of the statistic
I am measuring. Sometimes I do know the true CDF, but even in such cases, the
next (approximate) method is generally sufficient.

## The Approximate Method

If your statistic can be phrased as a sum, then its distribution approaches a
normal distribution.[^fnote2] This means that you can solve the above equations
for a normal CDF rather than (in the case above) a binomial CDF. Since a
binomial $$B(n,p)$$ approaches a normal with mean $$\mu=np$$ and variance
$$\sigma^2=np(1-p)$$, we just need to know when the CDF of a $$N(np,
np(1-p))$$ is equal to $$\alpha/2$$ (for the lower bound; getting the upper
bound is similar).

How does this help? It helps because, for a normal distribution, we've already
solved the above equations to find lower and upper bounds. In particular, the inverval
$$[\mu-\sigma,\mu+\sigma]$$, also called a $$1\sigma$$-interval, covers about
68% of the mass (probability) of the normal PDF. A table below indicates the
probability mass contained in various symmetric intervals on a normal
distribution:

| Inverval | Width[^fnote3] | Coverage |
|-------|----| ----|
| $$[\mu-\sigma,\mu+\sigma]$$ | $$1\sigma$$ | 0.683 |
| $$[\mu-2\sigma,\mu+2\sigma]$$ | $$2\sigma$$ | 0.954 |
| $$[\mu-3\sigma,\mu+3\sigma]$$ | $$3\sigma$$ | 0.997 |

Therefore, if we want an (approximate) 95% confidence interval on the
percentage of women in the population of our city, we can just do a two-sigma
interval. The distribution of the parameter $$p$$ is calculated by dividing the
binomial variable by $$n$$, so the mean is $$\mu= p$$ and the variance is
$$\sigma^2 = p(1-p)/n$$.[^fnote4] In our case, $$p=5/8$$, so our confidence
interval is $$5/8 \pm 15/1280 \approx 0.625 \pm 0.0117$$.

## The Bootstrap Method

The previous approach relies on the accuracy of approximating our statistic's
distribution by a normal distribution. Bootstrapping is a pragmatic, flexible
approach to calculating confidence intervals, which makes no assumptions on the
underlying statistics we are calculating. We'll go into more detail on
bootstrapping in general below, so we'll be pretty brief here.

The basic idea is that, once you have bootstrapped an empirical distribution
for your statistic of interest (in the example above, this is the percentage of
the population that is women), then you can simply find the $$\alpha/2$$ and
$$1-\alpha/2$$ percentiles, which then become your confidence interval. Of
course, in the case above, we would expect our empirical bootstrapped
distribution to converge to a normal with mean $$\mu= p$$ and variance
$$\sigma^2 = p(1-p)/n$$. However, we can reasonably calculate percentiles
_regardless_ of what the empirical distribution is; this is why bootstrapping
confidence intervals are so flexible.

As you'll see below, the downside of bootstrapping confidence intervals is that
it requires some computation. The amount of computation required can be
anywhere from trivial to daunting, depending on how many samples you want in
your empirical distribution.

**Overall, I would recommend using the approximate method, or bootstrapping if
you aren't confident that your statistic is normally distributed.** Of course,
the central limit theorem can provide some guarantees about the asympototic
distribution of certain statistics, so it's worth thinking through whether that
applies to your situation, or not.

## Other Topics in Confidence Intervals

- **What is the definition of a confidence interval?** This is a bit more
  technical, but it's essential to know that it is **not** "there is a 95%
  probability that the true parameter is in this range." Actually, what it
  means is that "if you reran the experiment many times, then 95% of the time,
  your statistic would fall in this range."
  
- **How would this change if you wanted a _one-sided_ confidence interval?**
  This one isn't too bad - you just solve either $$CDF(n,p_l) = \alpha$$ or
  $$CDF(n,p_u) = 1-\alpha$$ for a lower- or upper-bounded interval,
  respectively.

# Bootstrapping

As a data scientist, you have to have a good understanding of bootstrapping. It
is a technique that allows you to get insight into the quality of your
estimates, based only on the data you have. To understand it, let's look
through an example.

In the last section, we samples 400 people in an effort to understand what
percentage of a city's population identified as female. We got an estimate that
was $$5/8$$. This estimate it itself a random variable; if we had sampled
different people, we might have ended up with a different number. What if we
want to know the distribution of this estimate? How would we go about getting
that?

Well, the obvious way is to go out and sample 400 more people, and repeat this
over and over again, until we have many such fractional estimates. But what if
we don't have access to sampling more people? The natural thing is to think
that we're out of luck - without the ability to sample further, we can't
actually understand more about the distribution of our parameter (ignoring, for
the moment, that we have lots of theoretical knowledge about it via the CLT).

The idea behind bootstrapping is simple. Sample from the data you already have,
with replacement, a new sample of 400 people. This will give you an estimate of
the female fraction that is distinct from your original estimate, due to the
replacement in your sampling. You can repeat this process as many times as
you like; you will then get an empirical distribution whic approaches the true
distribution of the statistic.[^fnote4]

Bootstrapping has the advantage of belig flexible, although it does have its
limitations. Rather than get too far into the weeds, I'll just point you to the
[Wikipedia article on bootstrapping][2]. There are also tons of resources about
this subject online. Try coding it up for yourself! By the time you're
interviewing, you should be able to write a bootstrapping algorithm quite
easily.

[Machine Learning Mastery][9] has a good introduction to bootstrapping that
uses the scikit-learn API. [Towards Data Science][10] codes it up directly in
NumPy, which is a useful thing to know how to be able to do.

## Other Topics in Bootstrapping

- **When would you _not_ want to use bootstrapping?** Well, it might not be
  feasible when it is very costly to calculate your sample statistic. To get
  accurate estimates you'll need to calculate your statistic thousands of
  times, so it might not be feasible if it takes minutes or hours to calculate
  a single sample. Also, it is often difficult to get strong theoretical
  guarantees about probabilities based on bootstrapping, so if you need a
  highly statistically rigorous approach, you might be better served with
  something more analytical. 

# Linear Regression

Regression is the study of the relationship between variables; for example, we
might wish to know how the weight of a person relates to their height. _Linear_
regression assumes that your input (height, or $$h$$) and output (weight, or
$$w$$) variables are _linearly related_, with slope $$\beta_1$$, intercept
$$\beta_0$$, and noise $$\epsilon$$.

$$w = \beta_1\cdot h + \beta_0 + \epsilon.$$

A linear regression analysis helps the user discover the $$\beta$$s in the
above equation. This is just the simplest application of LR; in reality, it is
quite flexible and can be used in a number of scenarios. 

Linear regression is another large topic that I can't really do justice to in
this article. Instead, I'll just go through some of the common topics, and
introduce the questions you should be able to address. As is the case with most
of these topics, you can look at the [MIT Statistics & Probability course][1]
for a solid academic introduction to the subject. You can also dig through [the
Wikipedia article][3] to get a more in-depth picture.

## Calculating a Linear Regression

Rather than go through an example here, I'll just refer you to the many
available guides that show you how to do this in code. Of course, you could do
it in raw NumPy, solving the normal equations explicitly, but I'd recommend
using scikit-learn or statsmodels, as they have much nicer interfaces, and give
you all sorts of additional information about your model ($$r^2$$, $$p$$-value,
etc.)

[Real Python][7] has a good guide to coding this up - see the section "Simple
Linear Regression with scikit-learn." [GeeksForGeeks][8] does the solution in
raw NumPy; the equations won't be meaningful for you until you read up on the
normal equation and how to analytically solve for the optimal LR
coefficients. If you want something similar in R, or Julia, or MATLAB,[^fnoted]
then I'm sure it's out there, you'll just have to go do some Googling to find
it.

## A Statistical View 

This subject straddles the boundary between statistics and machine-learning. It
has been quite thoroughly studied from a statistical point of view, and there
are some iportant results that you should be familiar with when thinking about
linear regression from a statistical frame.[^fnotec]

Let's look back at our foundational model for linear regression. LR assumes
that your input $$x$$ and output $$y$$ are related via 

$$y_i = \beta_1\cdot x_i + \beta_0 + \epsilon_i,$$

where $$\epsilon_i$$ are i.i.d., distributed as $$N(0, \sigma^2)$$. Since the
$$\epsilon$$ are random variables, the $$\beta_j$$ are themselves random
variables. One important question is whether there is, in fact, any
relationship between our variables at all. If there is not, then we should
$$\beta_1$$ close to 0,[^fnoteb] but they will not ever be exactly zero. One important
statistical technique in LR is **doing a hypothesis test against the null
hypothesis that $$\beta_1 = 0$$**. When a package like scikit-learn returns a
"$$p$$-value of the regression", this is the $$p$$-value they are talking
about.

You can learn more about the statistics of LR by looking at the [MIT course
notes on the subject][6].

## Validaing Your Model

Once you've calculated your LR, you'd like to validate it. I'd generally go
through the following steps:

- Look at your $$r^2$$ value. Is it reasonably large? You can look at the
  $$p$$-value to see if it's difference from zero is statistically significant
  (see the section below). You can also look at the RMSE of your model, but
  this number is not scaled between 0 and 1, so a "good" RMSE is highly
  dependent on the units of your indepedent variable. 
- Plot your residuals, for each variable. The residual is just the input minus
  the value predicted by your model, a.k.a. the error of your model. Plotting
  each residual isn't really feasible if you have hundreds of independent
  variables, but it's a good idea if your data is small enough. You should be
  looking for "homoskedasticity" - that the variance of the error is uniform
  across the range of the independent variable. If it's not, then certain
  things you've calculated (for example, the $$p$$-value of your regression)
  are no longer valid. You might also see that your errors have a bias that
  changes as the $$x_i$$ changes; this means that there's some more complicated
  relationship between $$y$$ and $$x_i$$ that your regression did not pick up.
  
Some of the questions below address the assumptions of linear regression; you
should be familiar with them, and now how to test for them either before or
after the regression is performed, so that you can be confident that your model
is valid.

## Basic Questions on LR

Hopefully you've familiarized yourself with the basic ideas behind linear
regression. Here are some conceptual questions you should be able to answer.

- **How are the $$\beta$$s calculated?** Practically, you let the library
  you're using take care of this. But behind the scenes, generally it's solving
  the so-called "normal equations", which give you the optimal (highest
  $$r^2$$) parameters possible.  You can use gradient descent to approximate
  the optimal solution when the design matrix is too large to invert; this is
  available via the `SGDRegressor` model in scikit-learn.
  
- **How do you decide if you should use linear regression?** The best case is
  when the data is 2- or 3-dimensional; then you can just plot the data and see
  if it looks like "linear plus noise". However, if you have lots of
  independent variables, this isn't really an option. In such a case, you
  should look perform a linear regression analysis, and then look at the errors
  to verify that they look normally distributed and homoskedastic (constant
  variance).

- **What does the $$r^2$$ value of a regression indicate?** The $$r^2$$ value
  indicates "how much of the variance of the output data is explained by the
  regression." That is, your output data $$y$$ has some (sample) variance, just
  on its own. Once you discover the linear relationship and subtract it off,
  then the remaining error $$y - \beta_0 - \beta_1x$$ still has some variance,
  but hopefully it's lower - $$r^2$$ is one minus the ratio of the original to
  the remaining variance. When $$r^2=1$$, then your line is a perfect fit of
  the data, and there is no remaining error. It is often used to explain the
  "quality" of your fit, although this can be a bit treacherous - see
  [Anscombe's Quartet][5] for examples of very different situations with the
  same $$r^2$$ value.
  
- **What are the assumptions you make when doing a linear regression?** The
  Wikipedia article [addresses this point][4] quite thoroughly. This is worth
  knowing, because you don't just want to jump in and blindly do LR; you want
  to be sure it's actually a reasonable approach.
  
- **When is it a bad idea to do LR?** One case in which it is a bad idea to do
  linear regression when the relationship between your variables. [Anscombe's
  Quartet][5] is a particularly striking example of how the output of a linear
  regression analysis can look similar but in fact the quality of the analysis
  can be radically different. Beyond this, it is a bad idea to do LR whenever
  the assumptions of LR are violated by the data; see the above bullet for more
  info there.
  
- **Can you do linear regression on a nonlinear relationship?** In many cases,
  yes. What we need is for the model to be linear in the parameters $$\beta$$;
  if, for example, you are comparing distance and time for a constantly
  accelerating object $$d = 1/2at^2$$, and you want to do regression to
  discover the acceleration $$a$$, then you can just use $$t^2$$ as your
  independent variable. The model relating $$d$$ and $$t^2$$ is linear in the
  acceleration $$a$$, as required.
  
- **What does the "linear" in linear regression refer to?** This one might seem
  trivial, but it's a bit of a trick question; the relationship $$y =
  2\log(x)$$ might not appear linear, but in fact it can be obtained via a
  linear regression, by using $$\log(x)$$ as the input variables, rather than
  $$x$$. Of course, for this to work, you need to know ahead of time that you
  want to compare against $$\log(x)$$, but this can be discovered via
  trial-and-error, to some extent. So the "linear" _does_, as you'd expect,
  mean that the relationship between independent and dependent variable is
  linear, but you can always _change_ either of them and re-calculate your
  regression. 


## Handling Overfitting

[RealPython][7] has good images showing examples of over-fitting. You can
handle it by building into your model a "penalty" on the $$\beta_i$$s; that is,
tell your model "I want low error, **and** I don't want large coefficients.**
The balance of these preferences is determined by a parameter, often denoted by
$$\lambda$$. 

Since you have many $$\beta$$s, in general, you have to combine them in some
fashion. Two such ways to calculate the "overall badness" $$OB$$ are

$$OB = \sqrt{ \beta_1^2 + \beta_2^2 + \ldots + \beta_n^2 } $$

or 

$$OB = |\beta_1| + |\beta_2| + \ldots + |\beta_n|.$$

The first will tend to be emphasize outliers; that is, it is more sensitive to
single large $$\beta$$s. The second considers all the $$\beta$$s more
uniformly. If you use the first, it is called "ridge regression", and if you
use the second it is called "LASSO regression."

In mathematics, these denote the $$\ell_1$$ and $$\ell_2$$ norms of the vectors
of $$\beta$$s; you can in theory use $$\ell_p$$ norms for any $$p$$, even
$$p=0$$ (count the number of non-zero $$\beta$$s to get the overall badness) or
$$p=\infty$$ (take the largest $$\beta$$ as the overall badness). However, in
practice, LASSO and ridge regression are already implemented in common
packages, so it's easy to use them right out of the box.

## Logistic Regression

Logistic regression is a way of modifying linear regression models to get a
classification model; it will be covered in the machine learning section, so we
won't discuss it here.

# Bayesian Inference

Up until now this guide has primarily focused on frequentist topics in
statistics, such as hypothesis testing and the frequentist approach to
confidence intervals. There is an entire world of Bayesian statistical
inference, which differs significantly from the frequentist approach in both
philosophy and technique. I will only touch on the most basic application of
Bayesian reasoning in this guide. Some companies (Google, in particular) tend
to focus on advanced Bayesian skills in their data science interviews; if you
want to really learn the Bayesian approach, I'd reccomend [Gelman's book][11],
which is a classic in the field.

In this section, I will mostly defer to outside sources, who I think speak more
eloquently on the topic than I can.

## Bayesian vs Frequentist Statistics

It's worth being able to eloquently discuss the difference in philosophy and
approach between the two schools of statistics. I particularly like the
discussion in the MIT course notes. They state, more or less, that while the
Bayesians like to reason from Bayes theorem

$$P(H|D) = \frac{ P(D|H)P(H)}{P(D)},$$

the frequentist school thinks that "the probability of the hypothesis" is a
nonsense concept - it is not a well-founded probablistic value, in the sense
that there is no repeatable experiment you can run in which to gather relative
frequency counts and calculate probabilities. Therefore, the frequentists must
reason directly from $$P(D|H)$$, the probability of the data given the
hypothesis, which is just the $$p$$-value. The upside of this is that the
probabilistic interpretation of $$P(D|H)$$ is clean and unambiguous; the
downside is that it is easy to misinterpret.

If you want to know more about this, there are endless discussions of it all
over the internet. Like many such dichotomies (emacs vs. vim, overhand vs
underhand toilet paper, etc.) it is generally overblown - a working
statistician should be familiar with, and comfortable using, both frequentist
_and_ Bayesian techniques in their analysis.

## Basics of Bayes Theorem

Bayes theorem tells us how to update our belief in light of new evidence. You
should be comfortably applying Bayes theorem in order to answer basic
probability questions. The classic example is the "base rate fallacy": 

Consider a routine screening test for a disease. Suppose the frequency of the
disease in the population (base rate) is 0.5%. The test is highly accurate with
a 5% false positive rate and a 10% false negative rate. You take the test and
it comes back positive. What is the probability that you have the disease?

The answer is NOT 0.95, even though the test has a 5% false positive rate. You
should be able to clearly work through this problem, building probability
tables and using Bayes theorem to calculate the final answer.

The problem is worked through in the [MIT stats course readings][12] (see
Example 10), so I'll defer to them for the details.

## Calculating Posteriors

The above approach of calculating out all the probabilites by hand works
reasonbly well when there are only a few possible outcomes in the probability
space, but it doesn't scale well to large probability (discrete) probability
spaces, and won't work at all in continuous probability spaces. In such
situations, you're still fundamentally relying on Bayes theorem, but the way it
is applied looks quite different - you end up using sums and integrals to
calculate the relevant terms.

Again, I'll defer to the [MIT stats course readings][13] for more on
this. Readings 12 and 13 are the relevant ones here.

It's particularly useful to be familiar with the concept of **conjugate
priors**. In general, updating your priors involves computing an integral,
which as anyone who has taken calculus knows can be a pain in the ass. When
sampling from a distribution and estimating the parameters, there are certain
priors for which the updates based on successive samples work out to be very
simple.

For an example of this, suppose you're flipping a biased coin and trying to
figure out the bias. This is equivalent to sampling a binomial distribution and
trying to estimate the parameter $$p$$. If your prior is uniform (flat across
the interval $$[0,1]$$), then after $$N$$ flips, $$k$$ of which come up heads,
your posterior probability density on $$p$$ will be

$$f(p) \propto p^{k}((1-p)^{N-k}.$$

This is called a **$$\beta$$ distribution**. It is kind of magical that we can
calculate this without having to do any integrals - this is because the
$$\beta$$ distribution is "conjugate to" the binomial distribution. It's
important that we started out with a uniform distribution as our prior - if we
had chosen an arbitrary prior, the algebra might not have worked out as
nicely. In particular, if we start with a non-$$\beta$$ prior, then this trick
won't work, because our prior will not be conjuage to the binomial distribution.

The other important conjugate pair to know is that of the Gaussian
distribution; it is, in fact, conjuage to itself, so if you estimate the
parameters of a normal distribution, those estimates are themselves normal, and
updating your belief about the parameters based on new draws from the normal
distribution is as simple as doing some algebra.

There are many good resources available online and in textbooks discussing
conjuage priors; [Wikipedia][14] is a good place to start.

# Conclusion

This guide has focused on some of the basic aspects of statistics that get
covered in data science interviews. It is far from exhaustive - different
companies focus on different skills, and will therefore be asking you about
different statistical concepts and techniques.

Please let me know if you have any corrections to what I've said here. I'm far
from a statistician, so I'm sure that I've made lots of small (and some large)
mistakes!

Stay tuned for the rest of the study guide, which should be appearing in the
coming months. And finally, best of luck with your job search! It can be a
challenging, and even demoralizing experience; just keep learning, and don't
let rejection get you down. Happy hunting!




<!-------------------------------- FOOTER ----------------------------> 


[1]: https://ocw.mit.edu/courses/mathematics/18-05-introduction-to-probability-and-statistics-spring-2014/index.htm

[2]: https://en.wikipedia.org/wiki/Bootstrapping_(statistics)

[3]: https://en.wikipedia.org/wiki/Linear_regression

[4]: https://en.wikipedia.org/wiki/Linear_regression#Assumptions

[5]: https://en.wikipedia.org/wiki/Anscombe%27s_quartet

[6]: https://ocw.mit.edu/courses/mathematics/18-05-introduction-to-probability-and-statistics-spring-2014/readings/MIT18_05S14_Reading25.pdf

[7]: https://realpython.com/linear-regression-in-python/

[8]: https://www.geeksforgeeks.org/linear-regression-python-implementation/

[9]: https://machinelearningmastery.com/a-gentle-introduction-to-the-bootstrap-method/

[10]: https://towardsdatascience.com/an-introduction-to-the-bootstrap-method-58bcb51b4d60

[11]: https://www.goodreads.com/book/show/619590.Bayesian_Data_Analysis

[12]: https://ocw.mit.edu/courses/mathematics/18-05-introduction-to-probability-and-statistics-spring-2014/readings/MIT18_05S14_Reading3.pdf

[13]: https://ocw.mit.edu/courses/mathematics/18-05-introduction-to-probability-and-statistics-spring-2014/readings/

[14]: https://en.wikipedia.org/wiki/Conjugate_prior

[15]: https://en.wikipedia.org/wiki/Type_I_and_type_II_errors#Type_I_error

[^fnote1]: Of course, the actual statement is careful about the mode of
    convergence, and the fact that it is actually an appropriately-normalized
    version of the distribution that converges, and so on.
    
[^fnotea]: If this feels familiar, it's because it's (statistically speaking)
    the same problem we worked with in the section on hypothesis testing.
    
[^fnote2]: Again, we're being loose here - it has to have finite variance, and
    the convergence is only in a specific sense.
    
[^fnote3]: I'm being a little loose with definitions here - the width of a
    $$2\sigma$$ inverval is actually $$4\sigma$$, but I think most would still
    describe it using the phrase "two-sigma".

[^fnotec]: Some of the issues that arise here (for example, over- and
    under-fitting) have solutions that are more practical and less theoretical and
    statistical in nature - these will be covered in more depth in the machine
    learning portion of this guide, and so we don't go into too much detail in this
    section.
    
[^fnoteb]: $$\beta_0$$ just represents the difference in the mean of the two
    variables, so it could be non-zero even if the two are independent.
    
[^fnoted]: Why are you using MATLAB? Stop that. You're not in school anymore.


<!-- Wish we could put this in _includes/scripts.html. But it doesn't run from -->
<!-- there. It needs to be run at the bottom of the file, rather than at the   -->
<!-- top; perhaps that has something to do with it. Anyways, I'll just include -->
<!-- this chunk of HTML at the footer of all my posts, even though its fugly.  -->

<div id="disqus_thread"></div>
<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://pwills-com.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
